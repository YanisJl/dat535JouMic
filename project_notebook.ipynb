{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT620 - Taiwan dengue cases analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.rdd import PipelinedRDD\n",
    "from itertools import islice\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from typing import Callable, List, Any, Tuple\n",
    "from pyspark.sql import Row, SparkSession\n",
    "import time\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"project\") \\\n",
    "    .set(\"spark.master\", \"yarn\") \\\n",
    "    .set(\"spark.deploy.mode\", \"cluster\") \\\n",
    "    .set(\"spark.executor.instances\", \"3\") \\\n",
    "    .set(\"spark.executor.cores\", \"4\") \\\n",
    "    .set(\"spark.executor.memory\", \"3G\") \\\n",
    "    .set(\"spark.dynamicAllocation.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(\"hdfs:///project/unstructured_dengue_10.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the unstructured data into a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_quotation_marks(row: List[Any]):\n",
    "        row[0] = row[0].strip(\"'\")\n",
    "        row[-1] = row[-1].strip(\"'\")\n",
    "        return row\n",
    "\n",
    "\n",
    "data = data \\\n",
    "        .map(lambda row: row.split(\"' '\")) \\\n",
    "        .map(lambda row: remove_quotation_marks(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep the wanted features (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [0, 2, 3, 4, 5, 18, 19, 20]\n",
    "\n",
    "data = data.map(lambda row: [row[col] for col in selected_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the first line containing the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.mapPartitionsWithIndex(\n",
    "    lambda idx, it: islice(it, 1, None) if idx == 0 else it\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert columns datatypes (dates and integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datatypes(row: List[Any], convert_function: Callable[[Any], Any], idxs: List[int]) -> List[Any]:\n",
    "    for idx in idxs:\n",
    "        row[idx] = convert_function(row[idx]) if row[idx] != '' else None\n",
    "    return row\n",
    "\n",
    "\n",
    "def convert_to_datetime(input_str: str, format: str) -> datetime:\n",
    "    return datetime.strptime(input_str, format)\n",
    "\n",
    "\n",
    "datetime_format = '%Y/%m/%d'\n",
    "\n",
    "data = data \\\n",
    "        .map(lambda row: convert_datatypes(row, partial(convert_to_datetime,\n",
    "                                                    format=datetime_format),\n",
    "                                                    [0, 1])) \\\n",
    "        .map(lambda row: convert_datatypes(row, int, [-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an index to the data to later join with the clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = data.zipWithIndex()\n",
    "data = data.map(lambda row: row[0] + [row[1]])\n",
    "data_with_index = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the datetime columns to number of days by calculating differences between dates, binary columns into a binary numeric value (0/1) and ordinal columns into a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_timestamp(row: List[str], idx: int) -> List[str]:\n",
    "        row[idx] = time.mktime(row[idx].timetuple())\n",
    "        return row\n",
    "\n",
    "\n",
    "def get_days_since_row(row: List[Any], idx: int,\n",
    "                   date: int = None, date_idx: int = None) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Calculate number of days between specified date columns.\n",
    "\n",
    "    :param row: data row\n",
    "    :param idx: idx to be recalculated\n",
    "    :param date: date to be subtracted, only used if date_idx is not specified\n",
    "    :param date_idx: idx of the column with the date to be subtracted\n",
    "    :return: new row\n",
    "    \"\"\"\n",
    "    if row[idx] is None:\n",
    "        return row\n",
    "\n",
    "    if date_idx is not None:\n",
    "        date = row[date_idx]\n",
    "\n",
    "    if date is None:\n",
    "        raise Exception('Specify date or date_idx')\n",
    "\n",
    "    row[idx] = row[idx]- date\n",
    "    return row\n",
    "\n",
    "\n",
    "def convert_sex(row: List[Any], idx: int) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Convert values of \"sex\" column to number encoding\n",
    "    \"\"\"\n",
    "    row[idx] = 0 if row[idx] == 'M' else 1\n",
    "    return row\n",
    "\n",
    "\n",
    "def convert_age(row: List[Any], idx: int) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Convert values of \"age\" column to number encoding\n",
    "    \"\"\"\n",
    "    row[idx] = int(row[idx].split('-')[0].split('+')[0])\n",
    "    return row\n",
    "\n",
    "\n",
    "def convert_infected(row: List[Any], idx: int) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Convert values of \"imported\" column to number encoding\n",
    "    \"\"\"\n",
    "    row[idx] = 0 if row[idx] == 'N' else 1\n",
    "    return row\n",
    "\n",
    "\n",
    "data_clustering = data_with_index \\\n",
    "                    .map(lambda row: convert_to_timestamp(row, 1)) \\\n",
    "                    .map(lambda row: convert_to_timestamp(row, 0)) \\\n",
    "                    .map(lambda row: get_days_since_row(row, 1, date_idx=0)) \\\n",
    "                    .map(lambda row: convert_sex(row, 2)) \\\n",
    "                    .map(lambda row: convert_age(row, 3)) \\\n",
    "                    .map(lambda row: convert_infected(row, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache the intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clustering.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the number of possible values in categorical columns to the most frequent ones and one hot encode the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def restrict_to_values(row: List[Any], idx: int, values: List[str]) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Assign values not present in \"values\" to \"Other\"\n",
    "    \"\"\"\n",
    "    if row[idx] not in values:\n",
    "        row[idx] = 'Other'\n",
    "    return row\n",
    "\n",
    "\n",
    "def reduce_number_of_categories_for_column(data: PipelinedRDD,\n",
    "                                            idx: int,\n",
    "                                            limit: int = 500) -> Tuple[Any, List[str]]:\n",
    "    \"\"\"\n",
    "    Reduce number of categories in a column to only those that have occurences higher than limit\n",
    "    \"\"\"\n",
    "    counts = data.map(lambda row: (row[idx], 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    values_to_be_kept = (\n",
    "        counts\n",
    "        .map(lambda row: ('Other', row[1]) if row[1] < limit or row[0] == 'None' else row)\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        .map(lambda row: row[0])\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    data = data.map(lambda row: restrict_to_values(row, idx, values_to_be_kept))\n",
    "\n",
    "    return data, values_to_be_kept\n",
    "\n",
    "\n",
    "def one_hot(row: List[Any], idx: int, values: List[str]) -> List[Any]:\n",
    "    \"\"\"\n",
    "    One hot encodes selected column\n",
    "    \"\"\"\n",
    "    for value in values:\n",
    "        row.append(1 if row[idx] == value else 0)\n",
    "\n",
    "    del row[idx]\n",
    "    return row\n",
    "\n",
    "\n",
    "# reduce number of cathegories for counties column\n",
    "data_clustering, counties_to_be_kept = reduce_number_of_categories_for_column(data_clustering, 4)\n",
    "\n",
    "# reduce number of cathegories for countries column\n",
    "data_clustering, countries_to_be_kept = reduce_number_of_categories_for_column(data_clustering, 6)\n",
    "\n",
    "data_clustering = (\n",
    "    data_clustering\n",
    "    .map(lambda row: one_hot(row, 4, counties_to_be_kept))\n",
    "    .map(lambda row: one_hot(row, 5, countries_to_be_kept))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def normalize(data: PipelinedRDD) -> PipelinedRDD:\n",
    "    \"\"\"\n",
    "    Normalize all columns in the data to 0-1 besides the index column\n",
    "    \"\"\"\n",
    "    max_values = (\n",
    "        data\n",
    "        .reduce(lambda r1, r2: [max(c1, c2) for c1, c2 in zip(r1, r2)])\n",
    "        )\n",
    "\n",
    "    return data.map(lambda row: [x / max_values[i] if i != 6 else x for i, x in enumerate(row)])\n",
    "\n",
    "\n",
    "data_clustering = normalize(data_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all columns except \"Date_Onset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda row: row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_timestamp(datetime_val: datetime) -> float:\n",
    "    return time.mktime(datetime_val.timetuple())\n",
    "\n",
    "\n",
    "data = data.map(lambda x: convert_to_timestamp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of new sick persons per day. To do so, we are adding a column of ones so we can do a sum during the reducing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda x: [x, 1])\n",
    "data_regression = data.reduceByKey(lambda sum, current: sum + current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = data_clustering.map(lambda x: Row(*x))\n",
    "df_clustering = spark.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the clustering using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=df_clustering.columns, outputCol=\"features\")\n",
    "df = assembler.transform(df_clustering)\n",
    "\n",
    "model = KMeans(k=10, seed=1)\n",
    "model = model.fit(df)\n",
    "\n",
    "predictions = model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the cluster information with the unprocessed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_index = data_with_index.map(lambda x: Row(*x))\n",
    "df_with_index = spark.createDataFrame(rows_with_index)\n",
    "predictions = predictions.selectExpr(\"_7 as _9\", 'prediction')\n",
    "predictions = predictions.join(df_with_index, ['_9'])\n",
    "\n",
    "predictions = predictions.selectExpr(\n",
    "\"_9 as Id\",\n",
    "\"prediction\",\n",
    "\"_1 as Date_Onset\", \n",
    "\"_2 as Days_To_Notification\", \n",
    "\"_3 as Sex\", \n",
    "\"_4 as Age_Group\", \n",
    "\"_5 as County_living\", \n",
    "\"_6 as Imported\", \n",
    "\"_7 as Country_infected\", \n",
    "\"_8 as Number_of_confirmed_cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results into parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.write.mode('overwrite').parquet(\"hdfs:///project/clustering_results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing and create a DataFrame for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "split_percentage = 0.90\n",
    "\n",
    "df = spark.createDataFrame(data_regression)\n",
    "df = df.sort(\"_1\", ascending=True)\n",
    "df = df.withColumnRenamed(\"_2\", \"label\")\n",
    "\n",
    "train_df_size = int(split_percentage * df.count())\n",
    "df = df.sort(\"_1\", ascending=True)\n",
    "train = df.limit(train_df_size)\n",
    "test = df.subtract(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the regression algorithm using RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler().setInputCols(['_1',]).setOutputCol('features')\n",
    "train01 = assembler.transform(train)\n",
    "train02 = train01.select(\"features\",\"label\")\n",
    "\n",
    "lr = RandomForestRegressor()\n",
    "model = lr.fit(train02)\n",
    "\n",
    "test01 = assembler.transform(test)\n",
    "test02 = test01.select('features', 'label')\n",
    "regression_results = model.transform(test02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results into parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results.write.mode('overwrite').parquet(\"hdfs:///project/regression_results.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
